# Deploy LLM

This is a repository for a production-ready LLM deployment (in time)

## Components

- LLM serving with vLLM
- Extracting and serving statistics with Prometheus
- Exposing GPU metrics with dcgm-exporter
- Displaying statistics with Grafana dashboard

## Running

Run everything with `docker compose up -d`

